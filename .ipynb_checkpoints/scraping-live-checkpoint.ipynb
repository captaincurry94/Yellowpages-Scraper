{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short notebook to scrape data from German yellowpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import beautiful soup for scraping the html content\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#import webdriver to open interactive browser with python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "#time will be used to stop the script for short times\n",
    "import time\n",
    "\n",
    "#random will be used to simulate more 'human' behavior when interacting with the page\n",
    "import random\n",
    "\n",
    "#import pandas for data wrangling\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to your chrome driver for selenium --> here you must insert the path from your individual device!\n",
    "chromedriver_path = \"E:\\python_projects\\chromedriver.exe\"\n",
    "\n",
    "#set the industry that you want to scrape the results for\n",
    "searchterm = \"Kosmetikstudio\"\n",
    "\n",
    "#set the city or zip code that you want to search in\n",
    "region = \"Hamburg\"\n",
    "\n",
    "#OPTIONAL: set how far from your region you want to search in kilometers\n",
    "search_distance =\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start pulling html data from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if search_distance ==\"\":\n",
    "    url = f\"https://www.gelbeseiten.de/Suche/{searchterm}/{region}\"\n",
    "else:\n",
    "    url = f\"https://www.gelbeseiten.de/Suche/{searchterm}/{region}?umkreis={search_distance*1000}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to scroll in the browser using selenium -> some content is only loaded into the html when you scroll past it\n",
    "def fast_scroll_down(browser):\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "def slow_scroll_down(browser):\n",
    "    total_height = int(browser.execute_script(\"return document.body.scrollHeight\"))\n",
    "    for i in range(1, total_height, 6):\n",
    "        browser.execute_script(\"window.scrollTo(0, {});\".format(i))\n",
    "\n",
    "def fast_scroll_up(browser):\n",
    "    browser.find_element_by_tag_name('body').send_keys(Keys.CONTROL + Keys.HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not all possible businesses are shown at once. The page only shows 50 businesses and then a button must be pressed to load 10 more entries into the list of visible entries. \n",
    "\n",
    "Thus, we will use Selenium to click the \"load more\" button until the html contains all available entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the browser\n",
    "browser = webdriver.Chrome(chromedriver_path)\n",
    "#open the page\n",
    "browser.get(url)\n",
    "#give page time to load\n",
    "time.sleep(5)\n",
    "browser.find_element_by_id(\"cmpbntyestxt\").click() #accept cookie policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/551 entries are loaded\n",
      "50/551 entries are loaded\n",
      "50/551 entries are loaded\n",
      "60/551 entries are loaded\n",
      "60/551 entries are loaded\n",
      "80/551 entries are loaded\n",
      "90/551 entries are loaded\n",
      "100/551 entries are loaded\n",
      "110/551 entries are loaded\n",
      "110/551 entries are loaded\n",
      "120/551 entries are loaded\n",
      "130/551 entries are loaded\n",
      "140/551 entries are loaded\n",
      "150/551 entries are loaded\n",
      "160/551 entries are loaded\n",
      "170/551 entries are loaded\n",
      "180/551 entries are loaded\n",
      "190/551 entries are loaded\n",
      "200/551 entries are loaded\n",
      "210/551 entries are loaded\n",
      "220/551 entries are loaded\n",
      "230/551 entries are loaded\n",
      "240/551 entries are loaded\n",
      "250/551 entries are loaded\n",
      "260/551 entries are loaded\n",
      "270/551 entries are loaded\n",
      "280/551 entries are loaded\n",
      "290/551 entries are loaded\n",
      "300/551 entries are loaded\n",
      "310/551 entries are loaded\n",
      "320/551 entries are loaded\n",
      "330/551 entries are loaded\n",
      "340/551 entries are loaded\n",
      "350/551 entries are loaded\n",
      "360/551 entries are loaded\n",
      "370/551 entries are loaded\n",
      "380/551 entries are loaded\n",
      "390/551 entries are loaded\n",
      "400/551 entries are loaded\n",
      "410/551 entries are loaded\n",
      "420/551 entries are loaded\n",
      "430/551 entries are loaded\n",
      "440/551 entries are loaded\n",
      "450/551 entries are loaded\n",
      "460/551 entries are loaded\n",
      "470/551 entries are loaded\n",
      "480/551 entries are loaded\n",
      "490/551 entries are loaded\n",
      "500/551 entries are loaded\n",
      "510/551 entries are loaded\n",
      "520/551 entries are loaded\n",
      "530/551 entries are loaded\n",
      "540/551 entries are loaded\n",
      "550/551 entries are loaded\n",
      "551/551 entries are loaded\n",
      "551 entries are in current html body\n"
     ]
    }
   ],
   "source": [
    "displayed_article_count = int(browser.find_element_by_id('loadMoreGezeigteAnzahl').text)\n",
    "available_article_count = int(browser.find_element_by_id('loadMoreGesamtzahl').text)\n",
    "\n",
    "#keep loading new entries until all possible entries are loaded into the html\n",
    "while displayed_article_count < available_article_count:\n",
    "    try:\n",
    "        browser.find_element_by_id(\"mod-LoadMore--button\").click()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        displayed_article_count = int(browser.find_element_by_id('loadMoreGezeigteAnzahl').text) # reset count of displayed entries\n",
    "        print (f'{displayed_article_count}/{available_article_count} entries are loaded')\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(round(random.randint(5,17)**.5,1)) #wait between 2-4 seconds for page to load to simulate human click behavior\n",
    "\n",
    "# set DOMdocument variable with new html that containt all available article entries\n",
    "DOMdocument = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "#close browser\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not every entry in the loaded overview contains all information of a given business. Thus, we will have to open the respective detail page of each business to see all contact and additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [] #placeholder list for the articles\n",
    "\n",
    "#iterate through all articles and pull the available data - not every business has every data type associated!\n",
    "for article in DOMdocument.find_all('article'):\n",
    "    name,street_name,street_no,zip_code,city,phone,email,homepage,description,tags,detail_link = '','','','','','','','','','',''\n",
    "    \n",
    "    try:\n",
    "        detail_link = article.find('a')['href'] #get the link to the company detail page\n",
    "        soup = BeautifulSoup(requests.get(detail_link).content,'html.parser')\n",
    "        container = soup.find('div', class_='mod-Kontaktdaten__container')\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        name = container.find('h3').text\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        full_street = container.find_all('p')[0].text\n",
    "        street_no = ''.join([i for i in full_street if i in '0123456789'])\n",
    "        street_name = full_street.replace(street_no,'').strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        full_zip = container.find_all('p')[1].text\n",
    "        zip_code = ''.join([i for i in full_zip if i in '0123456789'])\n",
    "        city = full_zip.replace(zip_code,'').strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        phone = container.find('li',class_='contains-icon-telefon').find('a').text.replace('\\n','').replace('\\t','').strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        email = container.find('li',class_='contains-icon-email').find('a').text.replace('\\n','').replace('\\t','').strip()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        homepage = container.find('li',class_='contains-icon-homepage').find('a').text.replace('\\n','').replace('\\t','').strip()\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        description = soup.find('div',class_='mod-Beschreibung__wrapper').find('div').text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        tags = [i for i in soup.find_all('h2',class_='gc-text--h1') if i.text ==\"Stichworte\"][0].next_element.next_element.next_element.text.replace('\\n','').replace('\\t','').strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    articles.append([name,street_name,street_no,zip_code,city,homepage,phone,email,description,tags,detail_link]) #save the data to the list of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'Kosmetikstudio_in_Hamburg.xlsx'\n"
     ]
    }
   ],
   "source": [
    "#save the data in a dataframe\n",
    "df=pd.DataFrame(articles,columns=['name','street_name','street_no','zip_code','city','homepage','phone','email','description','tags','detail_link'])\n",
    "\n",
    "#generate the filename\n",
    "filename = f\"{searchterm}_in_{region}\"\n",
    "if search_distance != \"\":\n",
    "    filename += f\"_{search_distance}km\"\n",
    "filename\n",
    "\n",
    "#save the dataframe to an excel list\n",
    "df.to_excel(filename + '.xlsx',index=False)\n",
    "\n",
    "print(\"Data saved to '\" + filename + \".xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
